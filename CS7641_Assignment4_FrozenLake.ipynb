{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "392f315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "from gym import wrappers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    #obs = obs[0]\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = policy[obs]\n",
    "        obs, reward, done, info = env.step(int(action))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 2000):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return scores\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    for s in range(env.observation_space.n):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Value-iteration algorithm \"\"\"\n",
    "    v = np.zeros(env.observation_space.n)  # initialize value-function\n",
    "    max_iterations = 10000\n",
    "    eps = 1e-20\n",
    "    for i in range(max_iterations):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.observation_space.n):\n",
    "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.action_space.n)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be1a750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bdd6ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma = 1e-06\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  3.3333400000033304e-112\n",
      "gamma = 1e-05\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  6.666733333333344e-104\n",
      "gamma = 0.0001\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  6.667333366666673e-76\n",
      "gamma = 0.001\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  3.333333333333342e-70\n",
      "gamma = 0.01\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  3.3670033670067683e-40\n",
      "gamma = 0.1\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  3.404502519587621e-23\n",
      "gamma = 0.2\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  1.5320456752858595e-14\n",
      "gamma = 0.4\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  8.296870856409555e-11\n",
      "gamma = 0.8\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  7.003818434055201e-05\n",
      "gamma = 1.0\n",
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  0.8626666666666667\n"
     ]
    }
   ],
   "source": [
    "for i in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.4, 0.8, 1.0]:\n",
    "    print(\"gamma = \" + str(i))\n",
    "    env_name  = 'FrozenLake8x8-v1'\n",
    "    gamma = i\n",
    "    env = gym.make(env_name)\n",
    "    optimal_v = value_iteration(env, i);\n",
    "    policy = extract_policy(optimal_v, i)\n",
    "    policy_scores = evaluate_policy(env, policy, i, n=3000)\n",
    "    print('Policy average score = ', np.mean(policy_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f845970",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8198d6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 2357.\n",
      "Average time: 1.1271429061889648\n",
      "Average score =  0.8736666666666667\n"
     ]
    }
   ],
   "source": [
    "env_name  = 'FrozenLake8x8-v1'\n",
    "env = gym.make(env_name)\n",
    "start = time.time()\n",
    "optimal_v = value_iteration(env, gamma);\n",
    "print(\"Average time: \" + str(((time.time() - start))))\n",
    "policy = extract_policy(optimal_v, gamma)\n",
    "policy_scores = evaluate_policy(env, policy, gamma, n=3000)\n",
    "print('Average score = ', np.mean(policy_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783f03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 16))\n",
    "sns.heatmap(policy,  cmap=\"YlGnBu\", annot=True, cbar=False, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60e4e352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0\n",
      "8 3.0\n",
      "8 3.0\n",
      "0 1.0\n",
      "8 3.0\n",
      "0 1.0\n",
      "1 2.0\n",
      "9 3.0\n",
      "8 3.0\n",
      "0 1.0\n",
      "1 2.0\n",
      "2 2.0\n",
      "2 2.0\n",
      "10 3.0\n",
      "2 2.0\n",
      "10 3.0\n",
      "2 2.0\n",
      "10 3.0\n",
      "9 3.0\n",
      "8 3.0\n",
      "9 3.0\n",
      "10 3.0\n",
      "9 3.0\n",
      "1 2.0\n",
      "2 2.0\n",
      "2 2.0\n",
      "3 1.0\n",
      "2 2.0\n",
      "3 1.0\n",
      "2 2.0\n",
      "2 2.0\n",
      "2 2.0\n",
      "3 1.0\n",
      "2 2.0\n",
      "3 1.0\n",
      "4 2.0\n",
      "5 2.0\n",
      "6 2.0\n",
      "6 2.0\n",
      "14 3.0\n",
      "6 2.0\n",
      "6 2.0\n",
      "14 3.0\n",
      "15 2.0\n",
      "7 2.0\n",
      "7 2.0\n",
      "7 2.0\n",
      "7 2.0\n",
      "7 2.0\n",
      "15 2.0\n",
      "23 2.0\n",
      "31 2.0\n",
      "31 2.0\n",
      "31 2.0\n",
      "31 2.0\n",
      "39 2.0\n",
      "39 2.0\n",
      "47 2.0\n",
      "55 2.0\n",
      "55 2.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward:  1.0\n"
     ]
    }
   ],
   "source": [
    "def display_value_iteration(P, env = gym.make('FrozenLake8x8-v1')):\n",
    "    nb_states = env.observation_space.n\n",
    "    visited_states = np.zeros(nb_states).astype(bool)\n",
    "    visited_states[0] = 1\n",
    "    states_labels = np.where(P==0, '<', \n",
    "                              np.where(P==1, 'v', \n",
    "                                       np.where(P==2, '>', \n",
    "                                                np.where(P==3, '^', P)\n",
    "                                               )\n",
    "                                      )\n",
    "                             ) \n",
    "    desc = env.unwrapped.desc.ravel().astype(str)\n",
    "    colors = np.where(desc=='S','y',np.where(desc=='F','b',np.where(desc=='H','r',np.where(desc=='G','g',desc))))\n",
    "    states_labels = np.zeros(nb_states).astype(str)\n",
    "    states_labels[:] = ''\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    #env.render()\n",
    "    done = False\n",
    "    while done != True: \n",
    "        best_a = P[s] # select the best next action from the policy\n",
    "        states_labels[s] = '^' if best_a==0 else ('>' if best_a==1 else ('v' if best_a==2 else '<'))   \n",
    "        print(s, best_a)\n",
    "        s, rew, done, info = env.step(best_a) #take step using selected action\n",
    "        total_reward = total_reward + rew\n",
    "        visited_states[s] = 1 # mark the state as visited\n",
    "        #env.render()\n",
    "    ax = sns.heatmap(P.reshape(int(np.sqrt(nb_states)),int(np.sqrt(nb_states))), \n",
    "                 linewidth=0.5, \n",
    "                 annot=states_labels.reshape(int(np.sqrt(nb_states)),int(np.sqrt(nb_states))), \n",
    "                 cmap=list(colors),\n",
    "                 fmt = '',\n",
    "                 cbar=False)\n",
    "    plt.show()\n",
    "    print(\"Total Reward: \", total_reward)\n",
    "    \n",
    "# display heatmap for a 4x4 board\n",
    "display_value_iteration(policy, env = env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9de443f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma = 1e-06\n",
      "Value-iteration converged at iteration# 6862.\n",
      "Policy average score =  0.0\n",
      "gamma = 1e-05\n",
      "Policy average score =  0.0\n",
      "gamma = 0.0001\n",
      "Policy average score =  0.0\n",
      "gamma = 0.001\n",
      "Policy average score =  0.0\n",
      "gamma = 0.01\n",
      "Value-iteration converged at iteration# 4680.\n",
      "Policy average score =  0.0\n",
      "gamma = 0.1\n",
      "Value-iteration converged at iteration# 6311.\n",
      "Policy average score =  0.0\n",
      "gamma = 0.2\n",
      "Policy average score =  0.0\n",
      "gamma = 0.4\n",
      "Policy average score =  5.230918112822262e-42\n",
      "gamma = 0.8\n",
      "Policy average score =  0.0\n",
      "gamma = 1.0\n",
      "Policy average score =  0.0\n"
     ]
    }
   ],
   "source": [
    "for i in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.4, 0.8, 1.0]:\n",
    "    print(\"gamma = \" + str(i))\n",
    "    random_map = generate_random_map(size=20, p=0.8)\n",
    "    gamma = i\n",
    "    env = gym.make(\"FrozenLake-v1\", desc=random_map)\n",
    "    optimal_v = value_iteration(env, i);\n",
    "    policy = extract_policy(optimal_v, i)\n",
    "    policy_scores = evaluate_policy(env, policy, i, n=3000)\n",
    "    print('Policy average score = ', np.mean(policy_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86b165d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
